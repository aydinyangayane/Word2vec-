{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# My RAM kept crashing, that is why I could not complete the training"
      ],
      "metadata": {
        "id": "TTXRh1pKpEae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CnATe6eaLiry"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "from nltk.tokenize import word_tokenize\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, Dot, Flatten\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "n_QHtKCyLnjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text: str) -> str:\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(\"[^Ա-Ֆա-ֆ]+\", \" \", text)\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "BMUow-HJMaHY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file_path = \"corpus_100k.zip\"\n",
        "extracted_dir = \"./corpus_data\"\n",
        "if not os.path.exists(extracted_dir):\n",
        "    os.makedirs(extracted_dir)"
      ],
      "metadata": {
        "id": "n9exL4R-Nolo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(extracted_dir)"
      ],
      "metadata": {
        "id": "0n28b9rONqqc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_file_path = os.path.join(extracted_dir, 'corpus_100k')\n",
        "with open(corpus_file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    corpus_text = file.read()"
      ],
      "metadata": {
        "id": "IkpNSp1TNtoR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [preprocess_text(corpus_text)]"
      ],
      "metadata": {
        "id": "uq7RncKJNyee"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 50\n",
        "WINDOW_SIZE = 5\n",
        "VOCAB_SIZE = len(set(texts[0]))\n",
        "NUM_NEGATIVE_SAMPLES = 5\n",
        "BATCH_SIZE = 32"
      ],
      "metadata": {
        "id": "Jx-hMlLEN0iI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index = {word: idx for idx, word in enumerate(set(texts[0]))}"
      ],
      "metadata": {
        "id": "ePAepLceN43L"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_indices = [word_to_index.get(word, VOCAB_SIZE) for word in texts[0]]"
      ],
      "metadata": {
        "id": "lUEXX7t_dnGU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_pairs, labels = [], []\n",
        "for target_idx, target_word in enumerate(text_indices):\n",
        "    if target_word == VOCAB_SIZE:\n",
        "        continue\n",
        "\n",
        "    context_indices = [\n",
        "        word_to_index.get(text_indices[context_idx], VOCAB_SIZE)\n",
        "        for context_idx in range(max(0, target_idx - WINDOW_SIZE), min(target_idx + WINDOW_SIZE + 1, len(text_indices)))\n",
        "        if context_idx != target_idx and text_indices[context_idx] != VOCAB_SIZE]\n",
        "\n",
        "    if context_indices:\n",
        "        for context_word in context_indices:\n",
        "            word_pairs.append([target_word, context_word])\n",
        "            labels.append(1)\n",
        "            for _ in range(NUM_NEGATIVE_SAMPLES):\n",
        "                negative_word = np.random.randint(0, VOCAB_SIZE)\n",
        "                word_pairs.append([target_word, negative_word])\n",
        "                labels.append(0)"
      ],
      "metadata": {
        "id": "aUBvawTjYyME"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_pairs = np.array(word_pairs, dtype = np.int32)\n",
        "labels = np.array(labels, dtype = np.int32)"
      ],
      "metadata": {
        "id": "tqckvmJcnEZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_target = Input(shape = (1,))\n",
        "input_context = Input(shape = (1,))\n",
        "word_embedding = Embedding(input_dim = VOCAB_SIZE + 1, output_dim = EMBEDDING_DIM)\n",
        "target_embedding = word_embedding(input_target)\n",
        "context_embedding = word_embedding(input_context)\n",
        "dot_product = Dot(axes = 2)([target_embedding, context_embedding])\n",
        "flatten = Flatten()(dot_product)\n",
        "model = Model(inputs = [input_target, input_context], outputs = flatten)"
      ],
      "metadata": {
        "id": "3C1MJ63bU3kZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_loss(y_true, y_pred):\n",
        "    return tf.nn.sigmoid_cross_entropy_with_logits(labels = tf.cast(y_true, dtype = tf.float32), logits = y_pred)"
      ],
      "metadata": {
        "id": "Cjb_n-AyfeEe"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = 'adam', loss = custom_loss)"
      ],
      "metadata": {
        "id": "pwg95SZ2U4Bh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit([word_pairs[:, 0], word_pairs[:, 1]], labels, epochs = 10, batch_size = BATCH_SIZE)"
      ],
      "metadata": {
        "id": "8Sn-Z-rFfnLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_embeddings = model.get_layer('embedding').get_weights()[0]\n",
        "np.save(\"word_embeddings.npy\", word_embeddings)"
      ],
      "metadata": {
        "id": "FB3CHCY1VAdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_dimensions_with_tsne(word_embeddings):\n",
        "    tsne = TSNE(n_components = 2, random_state = 0)\n",
        "    vectors_tsne = tsne.fit_transform(word_embeddings)\n",
        "    return vectors_tsne"
      ],
      "metadata": {
        "id": "pGuyqfozVO29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectors_tsne = reduce_dimensions_with_tsne(word_embeddings)"
      ],
      "metadata": {
        "id": "lqWUX9l7VF6H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}